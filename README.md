# Reconocimiento de GÃ©nero por Voz

Un sistema de reconocimiento de gÃ©nero que utiliza Deep Learning con TensorFlow 2 para identificar el gÃ©nero de un hablante a partir de su audio.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un modelo de red neuronal profunda que puede determinar si una voz corresponde a un hombre o una mujer. El sistema utiliza caracterÃ­sticas espectrales extraÃ­das del audio (Mel-spectrograms) para realizar la clasificaciÃ³n.

## ğŸš€ CaracterÃ­sticas

- âœ… Reconocimiento en tiempo real usando micrÃ³fono
- âœ… AnÃ¡lisis de archivos de audio (formato WAV recomendado)
- âœ… Modelo preentrenado incluido
- âœ… Interfaz de lÃ­nea de comandos simple
- âœ… Probabilidades de confianza en las predicciones

## ğŸ“¦ Requisitos

### Dependencias principales:
- TensorFlow 2.x.x
- Scikit-learn
- NumPy
- Pandas
- PyAudio
- Librosa

### InstalaciÃ³n

1. **Clonar el repositorio:**
   ```bash
   git clone https://github.com/XtrullorEas/reconocimiento-de-genero-por-voz
   cd reconocimiento-de-genero-por-voz
   ```

2. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“ Estructura del Proyecto

```
ğŸ“¦ reconocimiento-de-genero-por-voz/
â”œâ”€â”€ ğŸ“„ test.py              # Script principal para predicciones
â”œâ”€â”€ ğŸ“„ utils.py             # Funciones auxiliares y modelo
â”œâ”€â”€ ğŸ“„ requirements.txt     # Dependencias
â”œâ”€â”€ ğŸ“„ balanced-all.csv     # Dataset con rutas y etiquetas
â”œâ”€â”€ ğŸ“„ README.md           # Este archivo
â”œâ”€â”€ ğŸ“ results/            # Modelo entrenado y caracterÃ­sticas
â”‚   â”œâ”€â”€ model.h5          # Modelo de TensorFlow entrenado
â”‚   â”œâ”€â”€ features.npy      # CaracterÃ­sticas extraÃ­das
â”‚   â””â”€â”€ labels.npy        # Etiquetas correspondientes
â”œâ”€â”€ ğŸ“ data/              # Datos de entrenamiento (archivos .npy)
â”‚   â”œâ”€â”€ cv-other-train/
â”‚   â”œâ”€â”€ cv-other-dev/
â”‚   â”œâ”€â”€ cv-other-test/
â”‚   â”œâ”€â”€ cv-valid-train/
â”‚   â”œâ”€â”€ cv-valid-dev/
â”‚   â””â”€â”€ cv-valid-test/
â”œâ”€â”€ ğŸ“ test-samples/      # Muestras de audio para pruebas
â”‚   â”œâ”€â”€ hombre.wav
â”‚   â”œâ”€â”€ mujer1.wav
â”‚   â””â”€â”€ mujer2.wav
â””â”€â”€ ğŸ“ dev/               # Archivos de desarrollo (opcional)
    â”œâ”€â”€ train.py          # Script para entrenar modelo
    â”œâ”€â”€ preparation.py    # Script para procesar datos
    â”œâ”€â”€ README.md         # DescripciÃ³n de archivos de desarrollo
    â””â”€â”€ LICENSE           # Archivo de licencia
```

## ğŸ¯ Uso

### 1. Reconocimiento usando tu voz (micrÃ³fono)

```bash
python test.py
```

El sistema te pedirÃ¡ que hables. Comienza a hablar cuando veas el mensaje "Por favor habla" y automÃ¡ticamente se detendrÃ¡ cuando detecte silencio.

### 2. AnÃ¡lisis de archivo de audio

```bash
python test.py --file "ruta/al/archivo.wav"
```

**Ejemplo:**
```bash
python test.py --file "test-samples/hombre.wav"
```

**Salida esperada:**
```
Resultado: hombre
Probabilidades:     Hombre: 96.36%    Mujer: 3.64%
```

### 3. Ver ayuda

```bash
python test.py --help
```

## ğŸ“Š Dataset

El proyecto utiliza el dataset [Mozilla's Common Voice](https://www.kaggle.com/mozillaorg/common-voice) con las siguientes caracterÃ­sticas:

- **Muestras filtradas:** Solo se incluyen muestras vÃ¡lidas etiquetadas
- **Dataset balanceado:** Igual nÃºmero de muestras masculinas y femeninas
- **CaracterÃ­sticas:** Mel-spectrograms extraÃ­dos de archivos de audio
- **Formato:** Archivos .npy con vectores de caracterÃ­sticas de longitud fija

## ğŸ§  Arquitectura del Modelo

El modelo utiliza una red neuronal profunda con la siguiente arquitectura:

```
Entrada (128 caracterÃ­sticas)
    â†“
Dense(256) + Dropout(0.3)
    â†“
Dense(256, ReLU) + Dropout(0.3)
    â†“
Dense(128, ReLU) + Dropout(0.3)
    â†“
Dense(128, ReLU) + Dropout(0.3)
    â†“
Dense(64, ReLU) + Dropout(0.3)
    â†“
Dense(1, Sigmoid)
    â†“
Salida (0=Mujer, 1=Hombre)
```

## ğŸ“ˆ Rendimiento

- **FunciÃ³n de pÃ©rdida:** Binary Crossentropy
- **Optimizador:** Adam
- **MÃ©trica:** Accuracy
- **TÃ©cnica de regularizaciÃ³n:** Dropout (0.3)

## ğŸ”§ Archivos del Proyecto

### Archivos Necesarios para Funcionamiento:
- âœ… `test.py` - Script principal
- âœ… `utils.py` - Funciones auxiliares
- âœ… `requirements.txt` - Dependencias
- âœ… `balanced-all.csv` - Dataset
- âœ… `results/model.h5` - Modelo entrenado
- âœ… `data/` - Datos de caracterÃ­sticas

### Archivos de Desarrollo (Carpeta `dev/`):
- ğŸ”§ `dev/train.py` - Para entrenar modelo desde cero
- ğŸ”§ `dev/preparation.py` - Para procesar archivos de audio originales
- ğŸ”§ `dev/LICENSE` - Archivo de licencia

**Nota:** Los archivos en la carpeta `dev/` son opcionales y solo necesarios para desarrollo avanzado.

## ğŸ› ï¸ Entrenamiento (Opcional)

Si deseas entrenar el modelo desde cero:

```bash
python dev/train.py
```

**Nota:** El modelo preentrenado ya estÃ¡ incluido en `results/model.h5`.

## ğŸ”¨ Desarrollo Avanzado

### Carpeta `dev/`

La carpeta `dev/` contiene herramientas adicionales para desarrollo:

- **`dev/train.py`** - Script para entrenar el modelo desde cero con tus propios datos
- **`dev/preparation.py`** - Script para procesar archivos de audio originales y extraer caracterÃ­sticas
- **`dev/LICENSE`** - InformaciÃ³n de licencia del proyecto

Estos archivos no son necesarios para el funcionamiento bÃ¡sico del reconocimiento de gÃ©nero, pero son Ãºtiles si quieres:
- Entrenar el modelo con nuevos datos
- Modificar la arquitectura del modelo
- Procesar tus propios archivos de audio

### PreparaciÃ³n de Datos Personalizados

Si tienes archivos de audio propios y quieres entrenar un modelo personalizado:

1. Organiza tus archivos segÃºn la estructura esperada
2. Ejecuta `python dev/preparation.py` para extraer caracterÃ­sticas
3. Ejecuta `python dev/train.py` para entrenar el modelo

## ğŸµ CaracterÃ­sticas de Audio Soportadas

El sistema extrae las siguientes caracterÃ­sticas espectrales:

- **MEL Spectrogram:** CaracterÃ­sticas principales utilizadas
- **MFCC:** Coeficientes Cepstrales en Frecuencias Mel
- **Chroma:** CaracterÃ­sticas tonales
- **Spectral Contrast:** Contraste espectral
- **Tonnetz:** RepresentaciÃ³n armÃ³nica

## ğŸ“ Notas Importantes

1. **Formato de audio recomendado:** WAV a 16kHz
2. **DuraciÃ³n:** El sistema funciona mejor con grabaciones de 2-10 segundos
3. **Calidad:** Audio claro sin ruido de fondo produce mejores resultados
4. **Idioma:** Entrenado principalmente con audio en inglÃ©s

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu caracterÃ­stica (`git checkout -b feature/nueva-caracteristica`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/nueva-caracteristica`)
5. Crea un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia especificada en el archivo `dev/LICENSE`.

## ğŸ”— Referencias

- [Tutorial original](https://www.thepythoncode.com/article/gender-recognition-by-voice-using-tensorflow-in-python)
- [Dataset Mozilla Common Voice](https://www.kaggle.com/mozillaorg/common-voice)
- [Librosa Documentation](https://librosa.github.io/)
- [TensorFlow Documentation](https://www.tensorflow.org/)

---

**Desarrollado con â¤ï¸ usando TensorFlow y Python**